#!/usr/bin/env python
#

# Author: James Robert White, james.dna.white@gmail.com

# This takes input about blast query sequences and the database and produces an 
# estimate of the number of CPU hours a job is expected to take. The program used
# to make the estimate is called cunningham located in opt-packages dir.

# The input is any number of key-value pairs. Those keys starting with 'input.' are 
# taken to correspond to input tags containg fasta files. 
# 'cunningham' is run on each file and the results are added and output to std out 

# A scaling factor can be modified to scale up or down the estimate depending on 
# the application.
 
#
# The input data currently needed is:
# input.*
# misc.PROGRAM (which blast program)
# misc.REP_DB
# params.SCALE_FACTOR - How much to scale the total number of queries, can be a float
#                       defaults to 1 if not specified

# Also, in case of unknown DB or tblastx or tblastn usage, we use original code, so include:
# cluster.CLUSTER_NAME
# params.AVG_QUERY_SEQ_LEN
# params.NUM_QUERY_SEQ



# Importing ************************************************************************
import sys
import os
from igs.utils import commands
from vappio.webservice import tag
# **********************************************************************************

#
# Some blast programs take the query and will translate it in multiple ways
# so this is the amount of work they add to the computation
PROGS = {'blastn': 1,
         'blastp': 1,
         'blastx': 1,
        }
# only some databases are supported by cunningham:
DBS  = {'clovr-refseqdb': 1,
       'clovr-cogdb': 1,
       'clovr-eggnogdb': 1,
       'clovr-keggdb': 1,
       'ncbi-nr': 1,
       'clovr-rrnadb': 1  
       }


def cunninghamIt(f):
    estimate = 0
    ret = []
    commands.runSingleProgramEx('/opt/cunningham/cunningham -S -Q ' + f + ' -D '+ kv['misc.REP_DB'] + ' -P ' +  kv['params.PROGRAM'], stdoutf=ret.append, stderrf=None)
     
    for r in ret:  
        if 'Runtime' not in r: continue
        _notimportant, estimate = r.split(':', 1) 
    
    return (float(estimate))


# *****************************************************************************
# code to use if cunningham can't handle it
# *****************************************************************************
WORK_FACTOR = {'blastn': 1,
               'blastp': 1,
               'blastx': 6,
               'tblastn': 6,
               'tblastx': 12
               }

# A very rough estimate of how many residues can be calculated per core per hour is 28k
# Since we are giving that a work factor of 6 we are going to estimate the blastn/blastp
# baseline for that is 28000 * WORK_FACTOR['blastx']
RESIDUES_PER_CORE_PER_HOUR = 28000 * WORK_FACTOR['blastx']

def calculateCPUHours(program, numSeqs, avgLen, scale):
    """
    + 1 because we always high-ball by an hour
    /scaleDown because we allow fo rthe total number of hours to be brought down by factor if the input DB
    is smaller than NR
    """
    return (WORK_FACTOR[program] * float(numSeqs * avgLen)/float(RESIDUES_PER_CORE_PER_HOUR)) * scale + 1


def bail():

    if kv['misc.REP_DB'] not in DBS: # then rescale the scaling factor
        tagData = tag.queryTag('localhost', 'local', [kv['misc.REP_DB']])
        size = sum([os.stat(f).st_size for f in tagData('files')])        
        kv['params.SCALE_FACTOR'] = size/4.9e9 # here we assume the size of the uncompressed (no .tgz) ncbi-nr database is 4.9 Gigabytes    
    
    kv['params.NUM_QUERY_SEQ'] = int(kv['params.NUM_QUERY_SEQ'])
    for i in ['params.SCALE_FACTOR',
              'params.AVG_QUERY_SEQ_LEN',
             ]:
        kv[i] = float(kv[i])
    #
    # Insert magic calculuations here
    totalHours = calculateCPUHours(kv['params.PROGRAM'],
                                   kv['params.NUM_QUERY_SEQ'],
                                   kv['params.AVG_QUERY_SEQ_LEN'],
                                   kv['params.SCALE_FACTOR'])

    sys.stdout.write('\n'.join(['pipeline.COMPUTED_CPU_HOURS=' + str(totalHours)]) + '\n')
    exit(0)

# **********************************************************************************************************
# **********************************************************************************************************

# BEGIN

# Ensure we are dealing with kv pairs
if sys.stdin.readline().strip() != 'kv':
    raise Exception('Header needs to be kv')

# Write our header out
sys.stdout.write('kv\n')

# Expected key value pairs
kv = {'params.PROGRAM': None,
      'misc.REP_DB': None,
      'params.AVG_QUERY_SEQ_LEN': 1,
      'params.NUM_QUERY_SEQ': 1,
      'params.SCALE_FACTOR': 1,
     }

# Tag recording
tags = []
for line in sys.stdin:
    sys.stdout.write(line.strip() + '\n')
    k, v = line.strip().split('=', 1)
    if k in kv:
        kv[k] = v
    if k.startswith('input.'):
        tags.append(v)

if [v for v in kv.values() if not v]:
    raise Exception('Not all expected values present in config')

if kv['params.PROGRAM'] not in WORK_FACTOR:
    raise Exception('Undefined blast program ' + kv['params.PROGRAM'] + ' try a different program.')

if kv['params.PROGRAM'] not in PROGS:
    bail()

if kv['misc.REP_DB'] not in DBS:
    bail()

estimates = []
for t in tags:
    tagFile = tag.queryTag('localhost', 'local', [t])
    for f in tagFile('files'):
        e = cunninghamIt(f)
        estimates.append(e)

totalHours  = float(sum(estimates))
totalHours  = float(kv['params.SCALE_FACTOR'])*totalHours
if totalHours < 1: 
    totalHours = 1.01
sys.stdout.write('\n'.join(['pipeline.COMPUTED_CPU_HOURS=' + str(totalHours)]) + '\n')

